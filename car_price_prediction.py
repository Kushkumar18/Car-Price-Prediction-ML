# -*- coding: utf-8 -*-
"""Car_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QksLrLZ7wTJkX--0v_O8sfSLxmEPBIQz
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
# %matplotlib inline
mpl.style.use('ggplot')

# Creating the dataframe named as car
car=pd.read_csv('car_price.csv')

#inspecting the 1st five rows
car.head()

#Checking the no.of rows and columns
car.shape

#Getting some information about the dataset
car.info()

#Data Cleaning
#Checking the number of missing values
car.isnull().sum()

"""Cleaning the dataset

"""

#year has many non-year values
car=car[car['year'].str.isnumeric()]

#year is in object. Change to integer
car['year']=car['year'].astype(int)

#Price has Ask for Price
car=car[car['Price']!='Ask For Price']

#Price has commas in its prices and is in object
car['Price']=car['Price'].str.replace(',','').astype(int)

#kms_driven has object values with kms at last.
car['kms_driven']=car['kms_driven'].str.split().str.get(0).str.replace(',','')

#It has nan values and two rows have 'Petrol' in them
car=car[car['kms_driven'].str.isnumeric()]
car['kms_driven']=car['kms_driven'].astype(int)

#fuel_type has nan values
car=car[~car['fuel_type'].isna()]
car.shape

#Changing car names. Keeping only the first three words
car['name']=car['name'].str.split().str.slice(start=0,stop=3).str.join(' ')

#Resetting the index of the final cleaned data
car=car.reset_index(drop=True)
#Cleaned Data
car

car.describe(include='all')

#Removing outliers
car=car[car['Price']<6000000]

"""Checking relationship of Company with Price"""

import seaborn as sns
plt.subplots(figsize=(15,7))
ax=sns.boxplot(x='company',y='Price',data=car)
ax.set_xticklabels(ax.get_xticklabels(),rotation=40,ha='right')
plt.show()

"""Relationship of Price with FuelType, Year and Company

"""

ax=sns.relplot(x='company',y='Price',data=car,hue='fuel_type',size='year',height=7,aspect=2)
ax.set_xticklabels(rotation=40,ha='right')

"""Extracting Training Data"""

X=car[['name','company','year','kms_driven','fuel_type']]
y=car['Price']

X

y.shape

"""Applying Train Test Split"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.metrics import r2_score

#Creating an OneHotEncoder object to contain all the possible categories
ohe=OneHotEncoder()
ohe.fit(X[['name','company','fuel_type']])

#Creating a column transformer to transform categorical columns
column_trans=make_column_transformer((OneHotEncoder(categories=ohe.categories_),['name','company','fuel_type']),
                                    remainder='passthrough')

#Linear Regression Model
lr=LinearRegression()

#Making a pipeline
pipe=make_pipeline(column_trans,lr)

#Fitting the model
pipe.fit(X_train,y_train)

y_pred=pipe.predict(X_test)

#Checking R2 Score
r2_score(y_test,y_pred)

#Finding the model with a random state of TrainTestSplit where the model was found to give almost 0.92 as r2_score
scores=[]
for i in range(1000):
    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.1,random_state=i)
    lr=LinearRegression()
    pipe=make_pipeline(column_trans,lr)
    pipe.fit(X_train,y_train)
    y_pred=pipe.predict(X_test)
    scores.append(r2_score(y_test,y_pred))

np.argmax(scores)

scores[np.argmax(scores)]

pipe.predict(pd.DataFrame(columns=X_test.columns,data=np.array(['Maruti Suzuki Swift','Maruti',2019,100,'Petrol']).reshape(1,5)))

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.1,random_state=np.argmax(scores))
lr=LinearRegression()
pipe=make_pipeline(column_trans,lr)
pipe.fit(X_train,y_train)
y_pred=pipe.predict(X_test)
r2_score(y_test,y_pred)

import pickle
pickle.dump(pipe,open('LinearRegressionModel.pkl','wb'))

pipe.predict(pd.DataFrame(columns=['name','company','year','kms_driven','fuel_type'],data=np.array(['Maruti Suzuki Swift','Maruti',2019,100,'Petrol']).reshape(1,5)))

from sklearn.metrics import f1_score
from sklearn.preprocessing import KBinsDiscretizer

# Example: Binning prices into 3 categories (low, medium, high)
bins = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
y_test_class = bins.fit_transform(y_test.values.reshape(-1, 1)).flatten()
y_pred_class = bins.transform(y_pred.reshape(-1, 1)).flatten()

# Calculate F1 score
f1 = f1_score(y_test_class, y_pred_class, average='weighted')
print(f"F1 Score: {f1}")